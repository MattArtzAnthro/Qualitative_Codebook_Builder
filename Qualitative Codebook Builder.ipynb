{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Qualitative Codebook Builder\n",
        "\n",
        "Created by [Matt Artz](https://www.mattartz.me/) ‚Äî Advancing AI Anthropology through computational approaches to qualitative research.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## What This Tool Does\n",
        "\n",
        "This notebook extracts and develops research codebooks from source materials (academic articles, reports, methodology guides) following established methodological best practices. Instead of manually reading through dozens of papers to identify theoretical constructs and build coding frameworks, you receive a structured, codebook with definitions, criteria, and examples extracted from your literature.\n",
        "\n",
        "The system analyzes your source documents and creates a maximum of 40 codes to prevent cognitive overload, with each code including comprehensive definitions, inclusion/exclusion criteria, and supporting examples from the original texts.\n",
        "\n",
        "## Key Features for Anthropological Research\n",
        "\n",
        "1. **Multi-Format Document Support**: PDF, DOCX, DOC, TXT, RTF, XLSX, CSV file processing\n",
        "2. **Theory-Guided Extraction**: Implements inductive, deductive, and hybrid coding approaches\n",
        "3. **Quality Assurance Framework**: Built-in validation and conceptual distinctness assessment\n",
        "4. **Semantic Versioning**: Full version control with changelog tracking\n",
        "5. **Multiple Export Formats**: CSV, JSON, Markdown, ATLAS.ti, NVivo compatible outputs\n",
        "6. **Methodological Rigor**: Following established best practices for codebook development\n",
        "7. **Interactive Configuration**: Adjustable parameters for different research approaches\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. **Configure Parameters**: Set extraction approach (inductive/deductive/hybrid), quality thresholds, and processing preferences\n",
        "2. **Upload Source Documents**: Process academic articles, reports, and methodology guides in multiple formats\n",
        "3. **Automated Code Extraction**: AI analysis identifies theoretical constructs, methodological approaches, and key concepts\n",
        "4. **Quality Refinement**: Merge similar codes, validate definitions, and ensure conceptual distinctness\n",
        "5. **Export Documentation**: Generate codebooks in multiple formats with usage guidelines and quality reports\n",
        "\n",
        "## Applications in Anthropological Practice\n",
        "\n",
        "This tool supports any research requiring content analysis‚Äîfrom dissertation fieldwork to applied research projects. It's particularly useful for computational analysis using the tools in my AI Anthropology Toolkit, comparative studies requiring standardized coding frameworks, and collaborative research where multiple team members need consistent theoretical foundations.\n",
        "\n",
        "## Methodological Positioning\n",
        "\n",
        "This tool represents a **computational anthropology** approach‚Äîusing AI to enhance rather than replace traditional literature review and codebook development. The extraction preserves the theoretical rigor that defines anthropological inquiry while addressing the practical challenges of analyzing large bodies of literature.\n",
        "\n",
        "**Important**: This tool extracts codes from source documents but does not apply them to research data.\n",
        "\n",
        "## Target Audience\n",
        "\n",
        "Designed for anthropologists and qualitative researchers developing coding frameworks‚Äîfrom graduate students building dissertation codebooks to research teams requiring standardized theoretical foundations for large-scale studies.\n",
        "\n",
        "## Technical Approach\n",
        "\n",
        "The system employs **semantic analysis and natural language processing** to identify theoretical constructs and methodological approaches within academic literature. Using configurable extraction approaches, it systematically processes documents to build comprehensive codebooks while maintaining quality through similarity detection, validation checks, and conceptual distinctness assessment.\n",
        "\n",
        "## Contributing to AI Anthropology\n",
        "\n",
        "This notebook contributes to the emerging field of AI Anthropology‚Äîwhich combines studying AI as cultural artifact, using AI to enhance ethnographic research, and applying anthropological insights to AI development (Artz, forthcoming). By open-sourcing these tools, this work advances the collective capacity of anthropologists to work effectively with computational methods.\n",
        "\n",
        "## AI Anthropology Toolkit\n",
        "\n",
        "This tool is part of a growing suite of computational resources for anthropological research:\n",
        "\n",
        "- **[Qualitative Codebook Builder](https://github.com/MattArtzAnthro/Qualitative_Codebook_Builder)** (this tool) - AI-assisted development of qualitative coding frameworks\n",
        "- **[Interview Transcript Semantic Chunker](https://github.com/MattArtzAnthro/Interview_Transcript_Semantic_Chunker)** - AI-assisted segmentation of interview transcripts\n",
        "- **[Coding and Thematic Analysis](https://github.com/MattArtzAnthro/Coding_and_Thematic_Analysis)** - AI-assisted coding and thematic analysis of qualtiative data\n",
        "\n",
        "*Additional tools will be added to this toolkit as they are developed.*\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. You may remix, adapt, and build upon the material for non-commercial purposes, provided you credit Matt Artz and link to the repository.\n",
        "\n",
        "**Full license details**: https://creativecommons.org/licenses/by-nc/4.0/\n",
        "\n",
        "## Attribution   \n",
        "\n",
        "If you use or adapt this project in your work, please cite:\n",
        "\n",
        "\n",
        "> Built with the Qualitative Codebook Builder (Matt Artz, 2025) ‚Äî https://github.com/MattArtzAnthro/Qualitative_Codebook_Builder\n",
        "\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this tool in your academic research, please cite:\n",
        "\n",
        "\n",
        "> Artz, Matt. 2025. Qualitative Codebook Builder. Software.\n",
        "Zenodo. https://doi.org/10.5281/zenodo.15808612\n",
        "\n",
        "\n",
        "## Refrences\n",
        "Artz, Matt. Forthcoming. ‚ÄúAI Anthropology: The Future of Applied Anthropological Practice.‚Äù In Routledge Handbook of Applied Anthropology, edited by Christina Wasson, Edward B. Liebow, Karine L. Narahara, Ndukuyakhe Ndlovu, and Alaka Wali. New York: Routledge.\n"
      ],
      "metadata": {
        "id": "6IK6WsT4-YXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Package Installation\n",
        "\n",
        "Install required Python packages and import necessary libraries for document processing, AI analysis, and codebook generation. Run this cell first to ensure all dependencies are available for the extraction pipeline."
      ],
      "metadata": {
        "id": "F1a9fC-MS5Ls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgihSTfVS3Wk"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install anthropic pandas numpy scikit-learn nltk PyPDF2 python-docx openpyxl\n",
        "!pip install sentence-transformers python-pptx striprtf\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import anthropic\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import time\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# NLP and text processing\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')  # Add this to fix potential NLTK issues\n",
        "\n",
        "# File handling\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "from striprtf.striprtf import rtf_to_text\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "\n",
        "# For semantic similarity in code refinement\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Version control\n",
        "import hashlib\n",
        "from copy import deepcopy\n",
        "\n",
        "print(\"‚úì All packages installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration and Parameters\n",
        "\n",
        "Configure codebook development parameters using interactive widgets to customize extraction strategy, quality thresholds, and processing preferences. Set API credentials and define research approach before beginning document analysis.\n"
      ],
      "metadata": {
        "id": "81CKSGIkioo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive Configuration System with Widgets\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "import anthropic\n",
        "\n",
        "class CodeEntry:\n",
        "    \"\"\"Structure for each code following methodological guidelines\"\"\"\n",
        "    def __init__(self):\n",
        "        self.label = \"\"  # ‚â§25 chars, alphanumeric\n",
        "        self.definition = \"\"  # One litmus sentence\n",
        "        self.inclusion_criteria = []  # When to use\n",
        "        self.exclusion_criteria = []  # When NOT to use\n",
        "        self.examples = []  # 1-2 archetypal quotes\n",
        "        self.notes = []  # Analytic decisions, date-stamped\n",
        "        self.source_documents = []  # Where code was found\n",
        "        self.frequency = 0\n",
        "        self.created_date = datetime.now()\n",
        "        self.last_modified = datetime.now()\n",
        "        self.version = \"1.0.0\"\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration following best practices from methodological literature\"\"\"\n",
        "    # Default values\n",
        "    ANTHROPIC_API_KEY = \"\"\n",
        "    MAX_CODE_LABEL_LENGTH = 25\n",
        "    MIN_DEFINITION_LENGTH = 20\n",
        "    MAX_INITIAL_CODES = 40\n",
        "    MIN_EXAMPLES_PER_CODE = 2\n",
        "    MIN_CODE_FREQUENCY = 2\n",
        "    SIMILARITY_THRESHOLD = 0.85\n",
        "    CHUNK_SIZE = 500\n",
        "    OVERLAP = 50\n",
        "    MODEL = \"claude-sonnet-4-20250514\"\n",
        "    MAX_TOKENS = 4000\n",
        "    TEMPERATURE = 0.3\n",
        "    OUTPUT_PATH = \"/content/codebook_outputs/\"\n",
        "    VERSION_PATH = \"/content/codebook_versions/\"\n",
        "    PURPOSE = \"Extract theoretical and methodological codes from academic literature\"\n",
        "    EPISTEMOLOGICAL_STANCE = \"pragmatic\"\n",
        "    CODING_STRATEGY = \"hybrid\"\n",
        "\n",
        "def create_configuration_interface():\n",
        "    \"\"\"Create interactive configuration interface using AI Anthropology Toolkit styling\"\"\"\n",
        "\n",
        "    # Instructions with consistent styling\n",
        "    instructions_html = \"\"\"\n",
        "    <div style='background-color: #E7ECEF; padding: 20px; border-radius: 10px; margin: 20px 0; border-left: 5px solid #274C77;'>\n",
        "    <h3 style='color: #274C77; margin-top: 0;'>üéØ Configure Codebook Development</h3>\n",
        "    <p><strong>Welcome to the Qualitative Codebook Builder!</strong> Configure your extraction parameters and research approach below.</p>\n",
        "    <div style='background-color: #A3CEF1; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 4px solid #6096BA;'>\n",
        "        <p style='color: #274C77; margin: 0; font-weight: bold;'>üî¨ What this tool does:</p>\n",
        "        <ul style='color: #274C77; margin: 10px 0;'>\n",
        "            <li>Extract theoretical constructs and methodological approaches from academic literature</li>\n",
        "            <li>Build comprehensive coding frameworks with definitions and examples</li>\n",
        "            <li>Generate quality-assured codebooks for qualitative research</li>\n",
        "            <li>Export in multiple formats compatible with NVivo, ATLAS.ti, and other tools</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Styling\n",
        "    style = {'description_width': '200px'}\n",
        "    layout = widgets.Layout(width='400px')\n",
        "\n",
        "    # API Configuration\n",
        "    api_header = widgets.HTML(\"<h3 style='color: #274C77;'>üîë API Configuration</h3>\")\n",
        "\n",
        "    api_key_widget = widgets.Password(\n",
        "        value=Config.ANTHROPIC_API_KEY,\n",
        "        placeholder='Enter your Anthropic API key',\n",
        "        description='Anthropic API Key:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    model_widget = widgets.Dropdown(\n",
        "        options=[\n",
        "            'claude-sonnet-4-20250514',\n",
        "            'claude-3-5-sonnet-20241022',\n",
        "        ],\n",
        "        value=Config.MODEL,\n",
        "        description='Claude Model:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    # Codebook Parameters\n",
        "    codebook_header = widgets.HTML(\"<h3 style='color: #274C77;'>üìö Codebook Parameters</h3>\")\n",
        "\n",
        "    max_codes_widget = widgets.IntSlider(\n",
        "        value=Config.MAX_INITIAL_CODES,\n",
        "        min=10,\n",
        "        max=100,\n",
        "        step=5,\n",
        "        description='Max Initial Codes:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    max_label_length_widget = widgets.IntSlider(\n",
        "        value=Config.MAX_CODE_LABEL_LENGTH,\n",
        "        min=15,\n",
        "        max=50,\n",
        "        step=5,\n",
        "        description='Max Label Length:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    min_definition_length_widget = widgets.IntSlider(\n",
        "        value=Config.MIN_DEFINITION_LENGTH,\n",
        "        min=10,\n",
        "        max=100,\n",
        "        step=5,\n",
        "        description='Min Definition Length:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    min_examples_widget = widgets.IntSlider(\n",
        "        value=Config.MIN_EXAMPLES_PER_CODE,\n",
        "        min=1,\n",
        "        max=5,\n",
        "        step=1,\n",
        "        description='Min Examples per Code:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    # Quality Thresholds\n",
        "    quality_header = widgets.HTML(\"<h3 style='color: #274C77;'>üéØ Quality Thresholds</h3>\")\n",
        "\n",
        "    min_frequency_widget = widgets.IntSlider(\n",
        "        value=Config.MIN_CODE_FREQUENCY,\n",
        "        min=1,\n",
        "        max=10,\n",
        "        step=1,\n",
        "        description='Min Code Frequency:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    similarity_threshold_widget = widgets.FloatSlider(\n",
        "        value=Config.SIMILARITY_THRESHOLD,\n",
        "        min=0.5,\n",
        "        max=0.95,\n",
        "        step=0.05,\n",
        "        description='Similarity Threshold:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    # Processing Parameters\n",
        "    processing_header = widgets.HTML(\"<h3 style='color: #274C77;'>‚öôÔ∏è Processing Parameters</h3>\")\n",
        "\n",
        "    chunk_size_widget = widgets.IntSlider(\n",
        "        value=Config.CHUNK_SIZE,\n",
        "        min=200,\n",
        "        max=1000,\n",
        "        step=50,\n",
        "        description='Chunk Size (words):',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    overlap_widget = widgets.IntSlider(\n",
        "        value=Config.OVERLAP,\n",
        "        min=10,\n",
        "        max=200,\n",
        "        step=10,\n",
        "        description='Chunk Overlap:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    # LLM Parameters\n",
        "    llm_header = widgets.HTML(\"<h3 style='color: #274C77;'>ü§ñ LLM Parameters</h3>\")\n",
        "\n",
        "    max_tokens_widget = widgets.IntSlider(\n",
        "        value=Config.MAX_TOKENS,\n",
        "        min=1000,\n",
        "        max=8000,\n",
        "        step=500,\n",
        "        description='Max Tokens:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    temperature_widget = widgets.FloatSlider(\n",
        "        value=Config.TEMPERATURE,\n",
        "        min=0.0,\n",
        "        max=1.0,\n",
        "        step=0.1,\n",
        "        description='Temperature:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    # Research Configuration\n",
        "    research_header = widgets.HTML(\"<h3 style='color: #274C77;'>üî¨ Research Configuration</h3>\")\n",
        "\n",
        "    purpose_widget = widgets.Textarea(\n",
        "        value=Config.PURPOSE,\n",
        "        placeholder='Describe the purpose of your codebook...',\n",
        "        description='Codebook Purpose:',\n",
        "        style=style,\n",
        "        layout=widgets.Layout(width='600px', height='80px')\n",
        "    )\n",
        "\n",
        "    epistemological_widget = widgets.Dropdown(\n",
        "        options=['positivist', 'interpretivist', 'critical', 'pragmatic'],\n",
        "        value=Config.EPISTEMOLOGICAL_STANCE,\n",
        "        description='Epistemological Stance:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    coding_strategy_widget = widgets.Dropdown(\n",
        "        options=['deductive', 'inductive', 'hybrid'],\n",
        "        value=Config.CODING_STRATEGY,\n",
        "        description='Coding Strategy:',\n",
        "        style=style,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    # Action Buttons\n",
        "    apply_button = widgets.Button(\n",
        "        description='‚úÖ Apply Configuration',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Apply current settings',\n",
        "        icon='check',\n",
        "        layout=widgets.Layout(width='220px', height='45px', margin='5px'),\n",
        "        style={'button_color': '#6096BA', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    test_api_button = widgets.Button(\n",
        "        description='üß™ Test API Key',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Test if API key is valid',\n",
        "        icon='key',\n",
        "        layout=widgets.Layout(width='160px', height='45px', margin='5px'),\n",
        "        style={'button_color': '#A3CEF1', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    reset_button = widgets.Button(\n",
        "        description='üîÑ Reset Defaults',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Reset to default values',\n",
        "        icon='refresh',\n",
        "        layout=widgets.Layout(width='160px', height='45px', margin='5px'),\n",
        "        style={'button_color': '#8B8C89', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    # Status output\n",
        "    status_output = widgets.Output()\n",
        "\n",
        "    # Event handlers\n",
        "    def apply_configuration(b):\n",
        "        with status_output:\n",
        "            status_output.clear_output()\n",
        "            try:\n",
        "                # Update Config class\n",
        "                Config.ANTHROPIC_API_KEY = api_key_widget.value\n",
        "                Config.MODEL = model_widget.value\n",
        "                Config.MAX_INITIAL_CODES = max_codes_widget.value\n",
        "                Config.MAX_CODE_LABEL_LENGTH = max_label_length_widget.value\n",
        "                Config.MIN_DEFINITION_LENGTH = min_definition_length_widget.value\n",
        "                Config.MIN_EXAMPLES_PER_CODE = min_examples_widget.value\n",
        "                Config.MIN_CODE_FREQUENCY = min_frequency_widget.value\n",
        "                Config.SIMILARITY_THRESHOLD = similarity_threshold_widget.value\n",
        "                Config.CHUNK_SIZE = chunk_size_widget.value\n",
        "                Config.OVERLAP = overlap_widget.value\n",
        "                Config.MAX_TOKENS = max_tokens_widget.value\n",
        "                Config.TEMPERATURE = temperature_widget.value\n",
        "                Config.PURPOSE = purpose_widget.value\n",
        "                Config.EPISTEMOLOGICAL_STANCE = epistemological_widget.value\n",
        "                Config.CODING_STRATEGY = coding_strategy_widget.value\n",
        "\n",
        "                # Initialize client\n",
        "                global client\n",
        "                client = anthropic.Anthropic(api_key=Config.ANTHROPIC_API_KEY)\n",
        "\n",
        "                # Create directories\n",
        "                os.makedirs(Config.OUTPUT_PATH, exist_ok=True)\n",
        "                os.makedirs(Config.VERSION_PATH, exist_ok=True)\n",
        "\n",
        "                print(\"‚úÖ Configuration applied successfully!\")\n",
        "                print(f\"üìä Max Codes: {Config.MAX_INITIAL_CODES}\")\n",
        "                print(f\"üîß Chunk Size: {Config.CHUNK_SIZE} words\")\n",
        "                print(f\"üéØ Strategy: {Config.CODING_STRATEGY}\")\n",
        "                print(f\"üß† Model: {Config.MODEL}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error applying configuration: {e}\")\n",
        "\n",
        "    def test_api_key(b):\n",
        "        with status_output:\n",
        "            status_output.clear_output()\n",
        "            try:\n",
        "                if not api_key_widget.value:\n",
        "                    print(\"‚ùå Please enter an API key first\")\n",
        "                    return\n",
        "\n",
        "                print(\"üîÑ Testing API connection...\")\n",
        "                test_client = anthropic.Anthropic(api_key=api_key_widget.value)\n",
        "\n",
        "                response = test_client.messages.create(\n",
        "                    model=model_widget.value,\n",
        "                    max_tokens=10,\n",
        "                    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
        "                )\n",
        "\n",
        "                print(\"‚úÖ API key is valid and working!\")\n",
        "                print(f\"ü§ñ Model: {model_widget.value}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå API test failed: {e}\")\n",
        "\n",
        "    def reset_configuration(b):\n",
        "        with status_output:\n",
        "            status_output.clear_output()\n",
        "            # Reset all widgets to defaults\n",
        "            api_key_widget.value = \"\"\n",
        "            model_widget.value = \"claude-sonnet-4-20250514\"\n",
        "            max_codes_widget.value = 40\n",
        "            max_label_length_widget.value = 25\n",
        "            min_definition_length_widget.value = 20\n",
        "            min_examples_widget.value = 2\n",
        "            min_frequency_widget.value = 2\n",
        "            similarity_threshold_widget.value = 0.85\n",
        "            chunk_size_widget.value = 500\n",
        "            overlap_widget.value = 50\n",
        "            max_tokens_widget.value = 4000\n",
        "            temperature_widget.value = 0.3\n",
        "            purpose_widget.value = \"Extract theoretical and methodological codes from academic literature\"\n",
        "            epistemological_widget.value = \"pragmatic\"\n",
        "            coding_strategy_widget.value = \"hybrid\"\n",
        "            print(\"üîÑ Configuration reset to defaults\")\n",
        "\n",
        "    # Bind events\n",
        "    apply_button.on_click(apply_configuration)\n",
        "    test_api_button.on_click(test_api_key)\n",
        "    reset_button.on_click(reset_configuration)\n",
        "\n",
        "    # Help documentation\n",
        "    help_html = \"\"\"\n",
        "    <div style='background-color: #A3CEF1; padding: 15px; border-radius: 5px; margin: 15px 0; border-left: 4px solid #6096BA;'>\n",
        "    <h4 style='color: #274C77; margin-top: 0;'>üìñ Configuration Guide</h4>\n",
        "    <div style='display: flex; gap: 20px; margin: 15px 0;'>\n",
        "        <div style='flex: 1;'>\n",
        "            <ul style='color: #274C77;'>\n",
        "                <li><strong>Max Initial Codes:</strong> Limits extracted codes to prevent cognitive overload (recommended: 20-40)</li>\n",
        "                <li><strong>Chunk Size:</strong> Number of words per text segment. Larger = more context, slower processing</li>\n",
        "                <li><strong>Similarity Threshold:</strong> How similar codes must be to trigger merge consideration (0.85 = 85% similar)</li>\n",
        "                <li><strong>Temperature:</strong> LLM creativity (0.0 = deterministic, 1.0 = creative)</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        <div style='flex: 1;'>\n",
        "            <ul style='color: #274C77;'>\n",
        "                <li><strong>Coding Strategy:</strong>\n",
        "                    <ul>\n",
        "                        <li><em>Deductive:</em> Extract known theoretical frameworks</li>\n",
        "                        <li><em>Inductive:</em> Discover emergent themes</li>\n",
        "                        <li><em>Hybrid:</em> Combine both approaches (recommended)</li>\n",
        "                    </ul>\n",
        "                </li>\n",
        "                <li><strong>Quality Thresholds:</strong> Minimum requirements for code inclusion and validation</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "    </div>\n",
        "    <p style='color: #274C77; margin: 0;'><strong>üí° Tip:</strong> Start with default settings and adjust based on your specific research needs.</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Layout sections\n",
        "    api_section = widgets.VBox([api_header, api_key_widget, model_widget])\n",
        "    codebook_section = widgets.VBox([codebook_header, max_codes_widget, max_label_length_widget,\n",
        "                                   min_definition_length_widget, min_examples_widget])\n",
        "    quality_section = widgets.VBox([quality_header, min_frequency_widget, similarity_threshold_widget])\n",
        "    processing_section = widgets.VBox([processing_header, chunk_size_widget, overlap_widget])\n",
        "    llm_section = widgets.VBox([llm_header, max_tokens_widget, temperature_widget])\n",
        "    research_section = widgets.VBox([research_header, purpose_widget, epistemological_widget, coding_strategy_widget])\n",
        "\n",
        "    buttons_section = widgets.HBox([apply_button, test_api_button, reset_button])\n",
        "\n",
        "    # Organize into two columns for better spacing\n",
        "    left_column = widgets.VBox([\n",
        "        api_section,\n",
        "        codebook_section,\n",
        "        quality_section\n",
        "    ])\n",
        "\n",
        "    right_column = widgets.VBox([\n",
        "        processing_section,\n",
        "        llm_section,\n",
        "        research_section\n",
        "    ])\n",
        "\n",
        "    main_container = widgets.VBox([\n",
        "        widgets.HBox([left_column, right_column], layout=widgets.Layout(gap='40px')),\n",
        "        buttons_section,\n",
        "        status_output\n",
        "    ])\n",
        "\n",
        "    # Display everything\n",
        "    display(HTML(instructions_html))\n",
        "    display(HTML(help_html))\n",
        "    display(main_container)\n",
        "\n",
        "    return {\n",
        "        'api_key': api_key_widget,\n",
        "        'model': model_widget,\n",
        "        'max_codes': max_codes_widget,\n",
        "        'chunk_size': chunk_size_widget,\n",
        "        'temperature': temperature_widget,\n",
        "        'strategy': coding_strategy_widget,\n",
        "        'purpose': purpose_widget,\n",
        "        'apply_button': apply_button,\n",
        "        'test_button': test_api_button\n",
        "    }\n",
        "\n",
        "# Create the interface\n",
        "print(\"üéõÔ∏è Loading Interactive Configuration...\")\n",
        "config_widgets = create_configuration_interface()"
      ],
      "metadata": {
        "id": "wtLfIRI4qzjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Upload and Processing\n",
        "\n",
        "Upload source documents in multiple formats and extract clean text for analysis. The system handles various file types including PDFs, Word documents, and spreadsheets while providing detailed processing statistics.\n",
        "\n"
      ],
      "metadata": {
        "id": "GCrS1VaClYHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File Upload and Processing with Styled Interface\n",
        "\n",
        "def create_document_upload_interface():\n",
        "    \"\"\"Create styled file upload interface for codebook source documents\"\"\"\n",
        "\n",
        "    # Instructions with consistent styling\n",
        "    instructions_html = \"\"\"\n",
        "    <div style='background-color: #E7ECEF; padding: 20px; border-radius: 10px; margin: 20px 0; border-left: 5px solid #274C77;'>\n",
        "    <h3 style='color: #274C77; margin-top: 0;'>üìö Upload Source Documents</h3>\n",
        "    <p><strong>Ready to upload your source materials!</strong> This tool processes academic articles, reports, and methodology guides to extract theoretical constructs and coding frameworks.</p>\n",
        "\n",
        "    <div style='display: flex; gap: 20px; margin: 15px 0;'>\n",
        "        <div style='flex: 1; background-color: #A3CEF1; padding: 15px; border-radius: 8px; border-left: 4px solid #6096BA;'>\n",
        "            <h4 style='color: #274C77; margin-top: 0;'>‚úÖ Supported Formats:</h4>\n",
        "            <ul>\n",
        "                <li><strong>.pdf</strong> - Academic articles and reports</li>\n",
        "                <li><strong>.docx/.doc</strong> - Word documents and manuscripts</li>\n",
        "                <li><strong>.txt</strong> - Plain text files and notes</li>\n",
        "                <li><strong>.rtf</strong> - Rich Text Format documents</li>\n",
        "                <li><strong>.csv/.xlsx</strong> - Spreadsheet data and tables</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        <div style='flex: 1; background-color: #A3CEF1; padding: 15px; border-radius: 8px; border-left: 4px solid #6096BA;'>\n",
        "            <h4 style='color: #274C77; margin-top: 0;'>üí° Best Practices:</h4>\n",
        "            <ul>\n",
        "                <li>Include methodology sections for method extraction</li>\n",
        "                <li>Upload theoretical papers for framework identification</li>\n",
        "                <li>Ensure PDFs have selectable text (not scanned images)</li>\n",
        "                <li>Include diverse sources for comprehensive coverage</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "    </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Upload button\n",
        "    upload_button = widgets.Button(\n",
        "        description='üì§ Choose Source Documents',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Click to select and upload your source documents',\n",
        "        icon='upload',\n",
        "        layout=widgets.Layout(width='300px', height='50px'),\n",
        "        style={'button_color': '#6096BA', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    # Clear button\n",
        "    clear_button = widgets.Button(\n",
        "        description='üóëÔ∏è Clear All Documents',\n",
        "        disabled=False,\n",
        "        button_style='',\n",
        "        tooltip='Remove all uploaded documents',\n",
        "        icon='trash',\n",
        "        layout=widgets.Layout(width='180px', height='40px'),\n",
        "        style={'button_color': '#8B8C89', 'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    # File info display\n",
        "    file_info = widgets.HTML(\n",
        "        value=\"<p style='color: #666; font-style: italic;'>No documents uploaded yet. Click 'Choose Source Documents' to begin.</p>\",\n",
        "        layout=widgets.Layout(width='100%')\n",
        "    )\n",
        "\n",
        "    # Upload progress/status\n",
        "    upload_output = widgets.Output()\n",
        "\n",
        "    # Global storage for uploaded documents\n",
        "    global uploaded_documents\n",
        "    uploaded_documents = {}\n",
        "\n",
        "    def handle_document_upload(b):\n",
        "        with upload_output:\n",
        "            upload_output.clear_output()\n",
        "\n",
        "            print(\"=\" * 60)\n",
        "            print(\"üìö SOURCE DOCUMENT UPLOAD\")\n",
        "            print(\"=\" * 60)\n",
        "            print(\"Supported formats: PDF, DOCX, DOC, TXT, RTF, CSV, XLSX\")\n",
        "            print(\"Please select your source documents...\")\n",
        "            print()\n",
        "\n",
        "            try:\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                if not uploaded:\n",
        "                    print(\"‚ùå No files were selected.\")\n",
        "                    return\n",
        "\n",
        "                print(f\"\\nüì• Processing {len(uploaded)} document(s)...\")\n",
        "\n",
        "                global uploaded_documents\n",
        "                uploaded_documents.clear()\n",
        "\n",
        "                successful_files = []\n",
        "                failed_files = []\n",
        "\n",
        "                for filename, file_content in uploaded.items():\n",
        "                    print(f\"\\nüîÑ Processing: {filename}\")\n",
        "\n",
        "                    try:\n",
        "                        # Determine file type and process\n",
        "                        file_ext = filename.lower().split('.')[-1]\n",
        "\n",
        "                        if file_ext == 'pdf':\n",
        "                            text = extract_pdf_text(io.BytesIO(file_content))\n",
        "                        elif file_ext in ['docx', 'doc']:\n",
        "                            text = extract_docx_text(io.BytesIO(file_content))\n",
        "                        elif file_ext == 'txt':\n",
        "                            text = file_content.decode('utf-8', errors='ignore')\n",
        "                        elif file_ext == 'rtf':\n",
        "                            text = rtf_to_text(file_content.decode('utf-8', errors='ignore'))\n",
        "                        elif file_ext == 'csv':\n",
        "                            df = pd.read_csv(io.BytesIO(file_content))\n",
        "                            text = extract_csv_text(df)\n",
        "                        elif file_ext in ['xlsx', 'xls']:\n",
        "                            df = pd.read_excel(io.BytesIO(file_content))\n",
        "                            text = extract_excel_text(df)\n",
        "                        else:\n",
        "                            print(f\"   ‚ùå Unsupported file format: {file_ext}\")\n",
        "                            failed_files.append(filename)\n",
        "                            continue\n",
        "\n",
        "                        # Validate extracted text\n",
        "                        if not text or len(text.strip()) < 100:  # Minimum 100 chars for academic docs\n",
        "                            print(f\"   ‚ùå Insufficient text extracted from {filename}\")\n",
        "                            failed_files.append(filename)\n",
        "                            continue\n",
        "\n",
        "                        # Store document\n",
        "                        uploaded_documents[filename] = text\n",
        "\n",
        "                        # Calculate statistics\n",
        "                        word_count = len(text.split())\n",
        "                        char_count = len(text)\n",
        "                        estimated_pages = char_count // 2000  # Rough estimate\n",
        "                        file_size = len(file_content)\n",
        "\n",
        "                        print(f\"   ‚úÖ Success!\")\n",
        "                        print(f\"      üìä {word_count:,} words\")\n",
        "                        print(f\"      üìä {char_count:,} characters\")\n",
        "                        print(f\"      üìä ~{estimated_pages} pages\")\n",
        "                        print(f\"      üìÅ File size: {file_size:,} bytes\")\n",
        "\n",
        "                        successful_files.append(filename)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"   ‚ùå Error processing {filename}: {e}\")\n",
        "                        failed_files.append(filename)\n",
        "\n",
        "                # Update display\n",
        "                if successful_files:\n",
        "                    print(f\"\\nüéâ Successfully processed {len(successful_files)} document(s)!\")\n",
        "\n",
        "                    # Create summary display\n",
        "                    info_html = \"<div style='background-color: #A3CEF1; padding: 15px; border-radius: 5px; border-left: 4px solid #6096BA;'>\"\n",
        "                    info_html += f\"<h4 style='color: #274C77; margin-top: 0;'>‚úÖ {len(successful_files)} Source Document(s) Uploaded</h4>\"\n",
        "\n",
        "                    total_words = 0\n",
        "                    for filename in successful_files:\n",
        "                        text = uploaded_documents[filename]\n",
        "                        word_count = len(text.split())\n",
        "                        char_count = len(text)\n",
        "                        total_words += word_count\n",
        "\n",
        "                        info_html += f\"<div style='margin: 10px 0; padding: 10px; background-color: #E7ECEF; border-radius: 3px; border-left: 3px solid #274C77;'>\"\n",
        "                        info_html += f\"<strong>üìÑ {filename}</strong><br>\"\n",
        "                        info_html += f\"<small style='color: #274C77;'>üìä {word_count:,} words ‚Ä¢ {char_count:,} characters ‚Ä¢ ~{char_count//2000} pages</small>\"\n",
        "                        info_html += \"</div>\"\n",
        "\n",
        "                    if len(successful_files) > 1:\n",
        "                        info_html += f\"<p style='color: #274C77;'><strong>üìà Total: {total_words:,} words across all documents</strong></p>\"\n",
        "\n",
        "                    info_html += \"<p style='color: #274C77;'><strong>‚úÖ Ready for codebook development! Continue to configuration and processing.</strong></p>\"\n",
        "                    info_html += \"</div>\"\n",
        "\n",
        "                else:\n",
        "                    info_html = \"<div style='background-color: #E7ECEF; padding: 15px; border-radius: 5px; border-left: 4px solid #8B8C89;'>\"\n",
        "                    info_html += \"<h4 style='color: #274C77; margin-top: 0;'>‚ùå No Documents Successfully Processed</h4>\"\n",
        "                    info_html += \"<p style='color: #274C77;'>Please check your file formats and try again. Ensure PDFs have selectable text.</p>\"\n",
        "                    info_html += \"</div>\"\n",
        "\n",
        "                if failed_files:\n",
        "                    print(f\"\\n‚ö†Ô∏è Failed to process {len(failed_files)} file(s): {', '.join(failed_files)}\")\n",
        "\n",
        "                file_info.value = info_html\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ùå Upload error: {e}\")\n",
        "                file_info.value = f\"<p style='color: red;'>‚ùå Upload failed: {e}</p>\"\n",
        "\n",
        "    def clear_documents(b):\n",
        "        with upload_output:\n",
        "            upload_output.clear_output()\n",
        "\n",
        "            global uploaded_documents\n",
        "            uploaded_documents.clear()\n",
        "\n",
        "            file_info.value = \"<p style='color: #666; font-style: italic;'>Documents cleared. Click 'Choose Source Documents' to start over.</p>\"\n",
        "            print(\"üóëÔ∏è All documents cleared successfully.\")\n",
        "\n",
        "    # Bind events\n",
        "    upload_button.on_click(handle_document_upload)\n",
        "    clear_button.on_click(clear_documents)\n",
        "\n",
        "    # Layout\n",
        "    buttons_container = widgets.HBox([upload_button, clear_button])\n",
        "\n",
        "    # Display interface\n",
        "    display(HTML(instructions_html))\n",
        "    display(buttons_container)\n",
        "    display(file_info)\n",
        "    display(upload_output)\n",
        "\n",
        "    return {\n",
        "        'upload_button': upload_button,\n",
        "        'clear_button': clear_button,\n",
        "        'file_info': file_info,\n",
        "        'output': upload_output\n",
        "    }\n",
        "\n",
        "# Keep all your existing extraction functions exactly the same\n",
        "def extract_pdf_text(file_content):\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    try:\n",
        "        pdf_reader = PyPDF2.PdfReader(file_content)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page_text = pdf_reader.pages[page_num].extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"PDF extraction error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_docx_text(file_content):\n",
        "    \"\"\"Extract text from DOCX file\"\"\"\n",
        "    try:\n",
        "        doc = Document(file_content)\n",
        "        text = \"\"\n",
        "        # Extract from paragraphs\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text += paragraph.text + \"\\n\"\n",
        "        # Extract from tables\n",
        "        for table in doc.tables:\n",
        "            for row in table.rows:\n",
        "                for cell in row.cells:\n",
        "                    text += cell.text + \" \"\n",
        "            text += \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"DOCX extraction error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_csv_text(df):\n",
        "    \"\"\"Extract text from CSV dataframe\"\"\"\n",
        "    text_columns = df.select_dtypes(include=['object']).columns\n",
        "    text = \"\"\n",
        "    for col in text_columns:\n",
        "        text += f\"Column: {col}\\n\"\n",
        "        text += \" \".join(df[col].dropna().astype(str).tolist()) + \"\\n\\n\"\n",
        "    return text\n",
        "\n",
        "def extract_excel_text(df):\n",
        "    \"\"\"Extract text from Excel dataframe\"\"\"\n",
        "    if isinstance(df, dict):  # Multiple sheets\n",
        "        text = \"\"\n",
        "        for sheet_name, sheet_df in df.items():\n",
        "            text += f\"Sheet: {sheet_name}\\n\"\n",
        "            text += extract_csv_text(sheet_df)\n",
        "    else:\n",
        "        text = extract_csv_text(df)\n",
        "    return text\n",
        "\n",
        "# Initialize the styled interface\n",
        "print(\"üìö Document Upload Interface Ready\")\n",
        "print(\"üëÜ Configure your settings above, then upload your source documents below!\")\n",
        "\n",
        "# Global variable for document storage\n",
        "uploaded_documents = {}\n",
        "\n",
        "# Create and display upload interface\n",
        "upload_interface = create_document_upload_interface()"
      ],
      "metadata": {
        "id": "tdzviaKzla9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Segmentation\n",
        "\n",
        "Prepare documents for AI analysis by splitting them into manageable chunks with controlled overlap. This maintains context across segment boundaries while ensuring optimal processing efficiency for the language model."
      ],
      "metadata": {
        "id": "TyyvKsbKlsw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This block provides utilities for splitting documents into manageable chunks\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks for processing.\n",
        "    Maintains context across chunk boundaries.\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_size = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_size = len(sentence.split())\n",
        "\n",
        "        if current_size + sentence_size > chunk_size and current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            # Keep last few sentences for overlap\n",
        "            overlap_sentences = int(overlap * len(current_chunk) / current_size)\n",
        "            current_chunk = current_chunk[-overlap_sentences:] if overlap_sentences > 0 else []\n",
        "            current_size = sum(len(s.split()) for s in current_chunk)\n",
        "\n",
        "        current_chunk.append(sentence)\n",
        "        current_size += sentence_size\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "print(\"‚úì Text chunking utilities loaded\")"
      ],
      "metadata": {
        "id": "4rGFF0q2lvlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research Scope Definition\n",
        "\n",
        "Define the codebook's purpose and analyze uploaded documents to understand their theoretical frameworks and methodological approaches. This step establishes the foundation for targeted code extraction based on document content."
      ],
      "metadata": {
        "id": "crzlLWF6s9BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This block defines the codebook's purpose and analyzes uploaded documents\n",
        "\n",
        "def define_codebook_purpose():\n",
        "    \"\"\"Step 1: Clarify purpose and scope following best practices\"\"\"\n",
        "\n",
        "    purpose_config = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"purpose\": Config.PURPOSE,\n",
        "        \"epistemological_stance\": Config.EPISTEMOLOGICAL_STANCE,\n",
        "        \"primary_use\": \"Extract theoretical constructs and methodological approaches from literature\",\n",
        "        \"intended_audience\": [\"Research team\", \"Peer reviewers\", \"Future researchers\"],\n",
        "        \"scope_limitations\": [\n",
        "            f\"Maximum {Config.MAX_INITIAL_CODES} initial codes to prevent overload\",\n",
        "            \"Focus on conceptual and methodological codes\",\n",
        "            \"Exclude purely descriptive or administrative codes\"\n",
        "        ],\n",
        "        \"research_questions\": [\n",
        "            \"What theoretical constructs are present in the literature?\",\n",
        "            \"What methodological approaches are discussed?\",\n",
        "            \"What key concepts require operational definitions?\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Save purpose statement\n",
        "    with open(f\"{Config.OUTPUT_PATH}codebook_purpose.json\", 'w') as f:\n",
        "        json.dump(purpose_config, f, indent=2)\n",
        "\n",
        "    return purpose_config\n",
        "\n",
        "def analyze_document_types(documents: Dict[str, str]) -> Dict:\n",
        "    \"\"\"Analyze uploaded documents to understand their nature - FULL DOCUMENT ANALYSIS\"\"\"\n",
        "\n",
        "    print(\"\\nAnalyzing document types and content...\")\n",
        "    document_analysis = {}\n",
        "\n",
        "    for doc_name, content in documents.items():\n",
        "        print(f\"  Analyzing: {doc_name}\")\n",
        "\n",
        "        # For very long documents, analyze in segments\n",
        "        segments = []\n",
        "        if len(content) > 10000:  # If document is very long\n",
        "            # Analyze beginning, middle, and end\n",
        "            segments = [\n",
        "                content[:3000],  # Beginning\n",
        "                content[len(content)//2 - 1500:len(content)//2 + 1500],  # Middle\n",
        "                content[-3000:]  # End\n",
        "            ]\n",
        "        else:\n",
        "            # Analyze the whole document if it's shorter\n",
        "            segments = [content]\n",
        "\n",
        "        combined_analysis = {\n",
        "            'document_type': [],\n",
        "            'domain': [],\n",
        "            'frameworks': [],\n",
        "            'methods': [],\n",
        "            'code_categories': []\n",
        "        }\n",
        "\n",
        "        # Analyze each segment\n",
        "        for i, segment in enumerate(segments):\n",
        "            analysis_prompt = f\"\"\"\n",
        "            Analyze this {'full document' if len(segments) == 1 else f'segment {i+1} of document'} to determine:\n",
        "            1. Document type (e.g., empirical article, theoretical paper, methodology guide, report)\n",
        "            2. Primary domain/field\n",
        "            3. Key theoretical frameworks mentioned\n",
        "            4. Methodological approaches discussed\n",
        "            5. Potential code categories to extract\n",
        "\n",
        "            Be comprehensive - identify ALL frameworks, methods, and concepts present.\n",
        "\n",
        "            Text:\n",
        "            {segment}\n",
        "\n",
        "            Return ONLY valid JSON without markdown formatting. Use this exact structure:\n",
        "            {{\n",
        "                \"document_type\": \"string\",\n",
        "                \"domain\": \"string\",\n",
        "                \"frameworks\": [\"list\", \"of\", \"frameworks\"],\n",
        "                \"methods\": [\"list\", \"of\", \"methods\"],\n",
        "                \"code_categories\": [\"list\", \"of\", \"categories\"]\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                response = client.messages.create(\n",
        "                    model=Config.MODEL,\n",
        "                    max_tokens=1500,\n",
        "                    temperature=0.2,  # Lower temperature for consistent formatting\n",
        "                    messages=[{\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": analysis_prompt\n",
        "                    }],\n",
        "                    timeout=120.0\n",
        "                )\n",
        "\n",
        "                # Clean and parse the response\n",
        "                raw_response_text = response.content[0].text\n",
        "                print(f\"  Raw API response for segment {i+1}: {raw_response_text[:200]}...\")\n",
        "\n",
        "                try:\n",
        "                    cleaned_response = raw_response_text.strip()\n",
        "\n",
        "                    # Remove markdown code blocks\n",
        "                    if cleaned_response.startswith(\"```json\"):\n",
        "                        cleaned_response = cleaned_response[7:]\n",
        "                    if cleaned_response.startswith(\"```\"):\n",
        "                        cleaned_response = cleaned_response[3:]\n",
        "                    if cleaned_response.endswith(\"```\"):\n",
        "                        cleaned_response = cleaned_response[:-3]\n",
        "\n",
        "                    cleaned_response = cleaned_response.strip()\n",
        "\n",
        "                    # Parse JSON\n",
        "                    segment_analysis = json.loads(cleaned_response)\n",
        "\n",
        "                    # Aggregate results\n",
        "                    if isinstance(segment_analysis.get('document_type'), str):\n",
        "                        combined_analysis['document_type'].append(segment_analysis['document_type'])\n",
        "                    if isinstance(segment_analysis.get('domain'), str):\n",
        "                        combined_analysis['domain'].append(segment_analysis['domain'])\n",
        "\n",
        "                    for key in ['frameworks', 'methods', 'code_categories']:\n",
        "                        if isinstance(segment_analysis.get(key), list):\n",
        "                            combined_analysis[key].extend(segment_analysis[key])\n",
        "\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"    JSON parsing error for segment {i+1}: {e}\")\n",
        "                    print(f\"    Raw response (first 200 chars): {raw_response_text[:200]}...\")\n",
        "                    continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    Error analyzing segment {i+1}: {str(e)}\")\n",
        "\n",
        "        # Consolidate analysis\n",
        "        document_analysis[doc_name] = {\n",
        "            'document_type': Counter(combined_analysis['document_type']).most_common(1)[0][0] if combined_analysis['document_type'] else 'Unknown',\n",
        "            'domain': Counter(combined_analysis['domain']).most_common(1)[0][0] if combined_analysis['domain'] else 'Unknown',\n",
        "            'frameworks': list(set(combined_analysis['frameworks'])),  # Unique frameworks\n",
        "            'methods': list(set(combined_analysis['methods'])),  # Unique methods\n",
        "            'code_categories': list(set(combined_analysis['code_categories'])),  # Unique categories\n",
        "            'document_length': len(content),\n",
        "            'segments_analyzed': len(segments)\n",
        "        }\n",
        "\n",
        "        print(f\"    Found {len(document_analysis[doc_name]['frameworks'])} frameworks, \"\n",
        "              f\"{len(document_analysis[doc_name]['methods'])} methods\")\n",
        "\n",
        "    # Save analysis\n",
        "    with open(f\"{Config.OUTPUT_PATH}document_analysis.json\", 'w') as f:\n",
        "        json.dump(document_analysis, f, indent=2)\n",
        "\n",
        "    return document_analysis"
      ],
      "metadata": {
        "id": "bYaD3Stvl4-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Code Extraction\n",
        "\n",
        "Extract theoretical constructs and methodological concepts from source documents using the configured coding strategy. The system identifies key terms, frameworks, and approaches while building comprehensive code definitions and examples.\n",
        "\n"
      ],
      "metadata": {
        "id": "frX-Fp0ftAmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This block extracts initial codes from documents using the specified coding approach\n",
        "\n",
        "def extract_initial_codes(documents: Dict[str, str],\n",
        "                         coding_strategy: str = \"hybrid\") -> Tuple[Dict[str, CodeEntry], List]:\n",
        "    \"\"\"\n",
        "    Step 3: Build initial code set using parallel deductive + inductive generation\n",
        "    \"\"\"\n",
        "\n",
        "    # Clear any existing global codebook state\n",
        "    global uploaded_documents\n",
        "\n",
        "    codebook = {}\n",
        "    extraction_log = []\n",
        "\n",
        "    print(f\"Starting fresh extraction with {len(documents)} documents...\")\n",
        "    print(f\"Using {coding_strategy} coding strategy\")\n",
        "\n",
        "    # Define extraction prompt based on strategy\n",
        "    if coding_strategy == \"inductive\":\n",
        "        prompt_template = \"\"\"\n",
        "        Using inductive coding, extract potential codes from the provided text.\n",
        "        Focus on:\n",
        "        - Theoretical concepts and constructs\n",
        "        - Methodological approaches\n",
        "        - Key terms that appear multiple times\n",
        "        - Conceptual frameworks\n",
        "\n",
        "        For each code provide:\n",
        "        - label: ‚â§25 characters, alphanumeric only, no spaces (use_underscores)\n",
        "        - definition: One clear sentence defining the concept\n",
        "        - example: Direct quote showing the concept\n",
        "        - context: Why this is a meaningful code\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Return ONLY valid JSON without markdown formatting. Structure as array:\n",
        "        [\n",
        "          {{\n",
        "            \"label\": \"code_name\",\n",
        "            \"definition\": \"definition text\",\n",
        "            \"example\": \"quote example\",\n",
        "            \"context\": \"context explanation\"\n",
        "          }}\n",
        "        ]\n",
        "        \"\"\"\n",
        "\n",
        "    elif coding_strategy == \"deductive\":\n",
        "        prompt_template = \"\"\"\n",
        "        Extract codes from the provided text based on established theoretical frameworks.\n",
        "        Look specifically for:\n",
        "        - Established theories (e.g., grounded theory, phenomenology)\n",
        "        - Standard methodological approaches\n",
        "        - Common analytical frameworks\n",
        "        - Disciplinary conventions\n",
        "\n",
        "        Format requirements:\n",
        "        - label: ‚â§25 characters, alphanumeric only\n",
        "        - definition: One litmus sentence\n",
        "        - example: Supporting quote\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Return ONLY valid JSON without markdown formatting. Structure as array:\n",
        "        [\n",
        "          {{\n",
        "            \"label\": \"code_name\",\n",
        "            \"definition\": \"definition text\",\n",
        "            \"example\": \"quote example\"\n",
        "          }}\n",
        "        ]\n",
        "        \"\"\"\n",
        "\n",
        "    else:  # hybrid - most common\n",
        "        prompt_template = \"\"\"\n",
        "        Extract codes from the provided text using a hybrid approach.\n",
        "\n",
        "        First, identify standard theoretical/methodological codes:\n",
        "        - Established frameworks and theories\n",
        "        - Research design elements\n",
        "        - Analytical approaches\n",
        "\n",
        "        Then, identify emergent codes unique to this text:\n",
        "        - Novel concepts introduced\n",
        "        - Specific constructs defined\n",
        "        - Unique methodological innovations\n",
        "\n",
        "        For each code:\n",
        "        - label: ‚â§25 characters, alphanumeric, no spaces\n",
        "        - definition: One sentence \"litmus test\" definition\n",
        "        - code_type: \"deductive\" or \"inductive\"\n",
        "        - example: Direct quote (50-150 words)\n",
        "        - inclusion: When to use this code\n",
        "        - exclusion: When NOT to use this code\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Return ONLY valid JSON without markdown formatting. Structure as array:\n",
        "        [\n",
        "          {{\n",
        "            \"label\": \"code_name\",\n",
        "            \"definition\": \"definition text\",\n",
        "            \"code_type\": \"deductive\",\n",
        "            \"example\": \"quote example\",\n",
        "            \"inclusion\": \"when to use\",\n",
        "            \"exclusion\": \"when not to use\"\n",
        "          }}\n",
        "        ]\n",
        "        \"\"\"\n",
        "\n",
        "    # Process each document\n",
        "    successful_extractions = 0\n",
        "    failed_extractions = 0\n",
        "\n",
        "    for doc_name, content in documents.items():\n",
        "        print(f\"\\nExtracting codes from: {doc_name}\")\n",
        "\n",
        "        # Chunk the document\n",
        "        chunks = chunk_text(content, Config.CHUNK_SIZE, Config.OVERLAP)\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if i % 3 == 0:\n",
        "                print(f\"  Processing chunk {i+1}/{len(chunks)} (Success: {successful_extractions}, Failed: {failed_extractions})\")\n",
        "\n",
        "            try:\n",
        "                response = client.messages.create(\n",
        "                    model=Config.MODEL,\n",
        "                    max_tokens=2000,\n",
        "                    temperature=0.2,  # Lower temperature for consistent formatting\n",
        "                    messages=[{\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt_template.format(text=chunk)\n",
        "                    }],\n",
        "                    timeout=120.0\n",
        "                )\n",
        "\n",
        "                # Clean and parse the response\n",
        "                raw_response_text = response.content[0].text\n",
        "\n",
        "                try:\n",
        "                    cleaned_response = raw_response_text.strip()\n",
        "\n",
        "                    # Remove markdown code blocks\n",
        "                    if cleaned_response.startswith(\"```json\"):\n",
        "                        cleaned_response = cleaned_response[7:]\n",
        "                    if cleaned_response.startswith(\"```\"):\n",
        "                        cleaned_response = cleaned_response[3:]\n",
        "                    if cleaned_response.endswith(\"```\"):\n",
        "                        cleaned_response = cleaned_response[:-3]\n",
        "\n",
        "                    cleaned_response = cleaned_response.strip()\n",
        "\n",
        "                    # Parse JSON\n",
        "                    extracted_codes = json.loads(cleaned_response)\n",
        "\n",
        "                    # Ensure it's a list\n",
        "                    if not isinstance(extracted_codes, list):\n",
        "                        print(f\"  Warning: Expected list but got {type(extracted_codes)} for chunk {i}\")\n",
        "                        continue\n",
        "\n",
        "                    # Process each extracted code\n",
        "                    codes_added_this_chunk = 0\n",
        "                    for code_data in extracted_codes:\n",
        "                        if not isinstance(code_data, dict):\n",
        "                            continue\n",
        "\n",
        "                        label = sanitize_code_label(code_data.get('label', f'UNKNOWN_CODE_{i}'))\n",
        "\n",
        "                        if not label or label == 'UNKNOWN_CODE':\n",
        "                            continue\n",
        "\n",
        "                        if label not in codebook:\n",
        "                            # Create new code entry\n",
        "                            code = CodeEntry()\n",
        "                            code.label = label\n",
        "                            code.definition = code_data.get('definition', 'No definition provided')\n",
        "                            code.source_documents.append(doc_name)\n",
        "                            code.frequency = 1\n",
        "\n",
        "                            # Add example\n",
        "                            example_text = code_data.get('example', 'No example provided')\n",
        "                            code.examples.append({\n",
        "                                'text': example_text[:500],  # Limit example length\n",
        "                                'source': doc_name,\n",
        "                                'chunk': i\n",
        "                            })\n",
        "\n",
        "                            # Add criteria if provided\n",
        "                            if 'inclusion' in code_data:\n",
        "                                code.inclusion_criteria.append(code_data['inclusion'])\n",
        "                            if 'exclusion' in code_data:\n",
        "                                code.exclusion_criteria.append(code_data['exclusion'])\n",
        "\n",
        "                            # Add note about extraction\n",
        "                            code.notes.append({\n",
        "                                'date': datetime.now().isoformat(),\n",
        "                                'note': f\"Extracted via {coding_strategy} coding from {doc_name}\",\n",
        "                                'context': code_data.get('context', '')\n",
        "                            })\n",
        "\n",
        "                            codebook[label] = code\n",
        "                            codes_added_this_chunk += 1\n",
        "                            print(f\"      Created new code: {label}\")\n",
        "\n",
        "                        else:\n",
        "                            # Update existing code\n",
        "                            codebook[label].frequency += 1\n",
        "                            if doc_name not in codebook[label].source_documents:\n",
        "                                codebook[label].source_documents.append(doc_name)\n",
        "\n",
        "                            # Add additional example if different enough\n",
        "                            if len(codebook[label].examples) < Config.MIN_EXAMPLES_PER_CODE:\n",
        "                                codebook[label].examples.append({\n",
        "                                    'text': code_data.get('example', 'No example provided')[:500],\n",
        "                                    'source': doc_name,\n",
        "                                    'chunk': i\n",
        "                                })\n",
        "                            print(f\"      Updated existing code: {label} (freq: {codebook[label].frequency})\")\n",
        "\n",
        "                        # Log extraction\n",
        "                        extraction_log.append({\n",
        "                            'timestamp': datetime.now().isoformat(),\n",
        "                            'document': doc_name,\n",
        "                            'chunk': i,\n",
        "                            'code': label,\n",
        "                            'action': 'created' if codebook[label].frequency == 1 else 'updated'\n",
        "                        })\n",
        "\n",
        "                    successful_extractions += 1\n",
        "                    if codes_added_this_chunk > 0:\n",
        "                        print(f\"    Added {codes_added_this_chunk} new codes from chunk {i}\")\n",
        "\n",
        "                except json.JSONDecodeError as e:\n",
        "                    failed_extractions += 1\n",
        "                    print(f\"  JSON parsing failed for chunk {i}: {e}\")\n",
        "                    print(f\"  Raw response (first 300 chars): {raw_response_text[:300]}...\")\n",
        "\n",
        "                    # Save problematic response for debugging\n",
        "                    with open(f\"{Config.OUTPUT_PATH}debug_chunk_{doc_name}_{i}.txt\", 'w') as f:\n",
        "                        f.write(f\"Original:\\n{raw_response_text}\\n\\nCleaned:\\n{cleaned_response}\")\n",
        "\n",
        "                time.sleep(0.5)  # Rate limiting\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_extractions += 1\n",
        "                print(f\"Error processing chunk {i}: {str(e)}\")\n",
        "\n",
        "    print(f\"\\nExtraction Summary:\")\n",
        "    print(f\"  Successful chunks: {successful_extractions}\")\n",
        "    print(f\"  Failed chunks: {failed_extractions}\")\n",
        "    print(f\"  Total codes extracted: {len(codebook)}\")\n",
        "    print(f\"  Unique code labels: {list(codebook.keys())}\")\n",
        "\n",
        "    # Verify no duplicates in the dictionary\n",
        "    if len(codebook) != len(set(codebook.keys())):\n",
        "        print(\"  ‚ö†Ô∏è WARNING: Duplicate detection in codebook keys!\")\n",
        "    else:\n",
        "        print(\"  ‚úÖ No duplicates detected in codebook\")\n",
        "\n",
        "    return codebook, extraction_log\n",
        "\n",
        "def sanitize_code_label(label: str) -> str:\n",
        "    \"\"\"Ensure code label meets requirements: ‚â§25 chars, alphanumeric only, with underscores between words\"\"\"\n",
        "    # First, handle spaces and convert them to underscores\n",
        "    label = label.replace(' ', '_')\n",
        "\n",
        "    # Insert underscores between camelCase words (e.g., ResponsibleAI -> Responsible_AI)\n",
        "    label = re.sub(r'([a-z])([A-Z])', r'\\1_\\2', label)\n",
        "\n",
        "    # Insert underscores between lowercase and numbers (e.g., AI2 -> AI_2)\n",
        "    label = re.sub(r'([a-zA-Z])([0-9])', r'\\1_\\2', label)\n",
        "\n",
        "    # Insert underscores between numbers and letters (e.g., 2AI -> 2_AI)\n",
        "    label = re.sub(r'([0-9])([a-zA-Z])', r'\\1_\\2', label)\n",
        "\n",
        "    # Remove non-alphanumeric characters (except underscores)\n",
        "    label = re.sub(r'[^a-zA-Z0-9_]', '_', label)\n",
        "\n",
        "    # Remove multiple underscores\n",
        "    label = re.sub(r'_+', '_', label)\n",
        "\n",
        "    # Trim to length\n",
        "    label = label[:Config.MAX_CODE_LABEL_LENGTH]\n",
        "\n",
        "    # Remove leading and trailing underscores\n",
        "    label = label.strip('_')\n",
        "\n",
        "    return label.upper()  # Uppercase for consistency"
      ],
      "metadata": {
        "id": "kcZZybC6tFp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Refinement and Validation\n",
        "\n",
        "\n",
        "Refine extracted codes through similarity detection and merging while assessing conceptual distinctness. This process ensures each code represents a unique theoretical construct and meets quality standards for research application."
      ],
      "metadata": {
        "id": "qjPj5DMItHd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This block handles the refinement of codes including merging similar codes\n",
        "\n",
        "def refine_and_assess_reliability(codebook: Dict[str, CodeEntry]) -> Tuple[Dict, Dict]:\n",
        "    \"\"\"\n",
        "    Step 4: Refine codes and assess conceptual reliability\n",
        "    Following guidelines for intercoder reliability in manual coding\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n=== Codebook Refinement Process ===\")\n",
        "\n",
        "    # 1. Prune rare codes\n",
        "    print(\"\\n1. Pruning rare codes...\")\n",
        "    refined_codebook = {}\n",
        "    for label, code in codebook.items():\n",
        "        if code.frequency >= Config.MIN_CODE_FREQUENCY:\n",
        "            refined_codebook[label] = code\n",
        "        else:\n",
        "            print(f\"  Removed: {label} (frequency: {code.frequency})\")\n",
        "\n",
        "    print(f\"  Retained {len(refined_codebook)} codes\")\n",
        "\n",
        "    # 2. Check for conceptual overlap and merge similar codes\n",
        "    print(\"\\n2. Checking for conceptual overlap...\")\n",
        "    merge_decisions = identify_similar_codes(refined_codebook)\n",
        "\n",
        "    # 3. Apply merges\n",
        "    for decision in merge_decisions:\n",
        "        if decision['should_merge']:\n",
        "            refined_codebook = merge_codes(\n",
        "                refined_codebook,\n",
        "                decision['code1'],\n",
        "                decision['code2'],\n",
        "                decision['merged_label'],\n",
        "                decision['merged_definition']\n",
        "            )\n",
        "\n",
        "    # 4. Ensure all codes have complete definitions\n",
        "    print(\"\\n3. Validating code completeness...\")\n",
        "    validation_report = validate_codes(refined_codebook)\n",
        "\n",
        "    # 5. Generate reliability metrics (conceptual distinctness)\n",
        "    print(\"\\n4. Assessing conceptual distinctness...\")\n",
        "    reliability_metrics = assess_conceptual_distinctness(refined_codebook)\n",
        "\n",
        "    return refined_codebook, {\n",
        "        'validation': validation_report,\n",
        "        'reliability': reliability_metrics,\n",
        "        'merge_decisions': merge_decisions\n",
        "    }\n",
        "\n",
        "def identify_similar_codes(codebook: Dict[str, CodeEntry]) -> List[Dict]:\n",
        "    \"\"\"Identify codes that may need merging\"\"\"\n",
        "\n",
        "    if len(codebook) < 2:\n",
        "        return []\n",
        "\n",
        "    # Create embeddings\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    code_labels = list(codebook.keys())\n",
        "\n",
        "    # Combine label, definition, and examples for embedding\n",
        "    code_texts = []\n",
        "    for label in code_labels:\n",
        "        code = codebook[label]\n",
        "        examples_text = \" \".join([ex['text'][:100] for ex in code.examples[:2]])\n",
        "        code_text = f\"{label}: {code.definition}. Examples: {examples_text}\"\n",
        "        code_texts.append(code_text)\n",
        "\n",
        "    embeddings = model.encode(code_texts)\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "    # Find similar pairs\n",
        "    merge_candidates = []\n",
        "\n",
        "    for i in range(len(code_labels)):\n",
        "        for j in range(i + 1, len(code_labels)):\n",
        "            if similarity_matrix[i][j] > Config.SIMILARITY_THRESHOLD:\n",
        "                # Use LLM to make merge decision\n",
        "                decision = evaluate_merge_decision(\n",
        "                    codebook[code_labels[i]],\n",
        "                    codebook[code_labels[j]],\n",
        "                    similarity_matrix[i][j]\n",
        "                )\n",
        "                merge_candidates.append(decision)\n",
        "\n",
        "    return merge_candidates\n",
        "\n",
        "def evaluate_merge_decision(code1: CodeEntry, code2: CodeEntry, similarity: float) -> Dict:\n",
        "    \"\"\"Use LLM to evaluate whether codes should be merged\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Evaluate whether these two codes should be merged based on conceptual overlap.\n",
        "\n",
        "    Similarity score: {similarity:.2f}\n",
        "\n",
        "    Code 1: {code1.label}\n",
        "    Definition: {code1.definition}\n",
        "    Example: {code1.examples[0]['text'] if code1.examples else 'No example'}\n",
        "\n",
        "    Code 2: {code2.label}\n",
        "    Definition: {code2.definition}\n",
        "    Example: {code2.examples[0]['text'] if code2.examples else 'No example'}\n",
        "\n",
        "    Consider:\n",
        "    - Are these conceptually distinct despite similar language?\n",
        "    - Would merging lose important nuance?\n",
        "    - Is one a subset of the other?\n",
        "\n",
        "    Return ONLY valid JSON without markdown formatting:\n",
        "    {{\n",
        "        \"should_merge\": true,\n",
        "        \"rationale\": \"explanation of decision\",\n",
        "        \"merged_label\": \"suggested_label_if_merging\",\n",
        "        \"merged_definition\": \"comprehensive definition if merging\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.messages.create(\n",
        "            model=Config.MODEL,\n",
        "            max_tokens=500,\n",
        "            temperature=0.2,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "\n",
        "        # Clean and parse response\n",
        "        raw_text = response.content[0].text\n",
        "\n",
        "        try:\n",
        "            cleaned_response = raw_text.strip()\n",
        "\n",
        "            # Remove markdown code blocks\n",
        "            if cleaned_response.startswith(\"```json\"):\n",
        "                cleaned_response = cleaned_response[7:]\n",
        "            if cleaned_response.startswith(\"```\"):\n",
        "                cleaned_response = cleaned_response[3:]\n",
        "            if cleaned_response.endswith(\"```\"):\n",
        "                cleaned_response = cleaned_response[:-3]\n",
        "\n",
        "            cleaned_response = cleaned_response.strip()\n",
        "\n",
        "            decision = json.loads(cleaned_response)\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON parsing error in merge evaluation: {e}\")\n",
        "            decision = {\n",
        "                'should_merge': False,\n",
        "                'rationale': 'Error parsing response'\n",
        "            }\n",
        "\n",
        "        decision['code1'] = code1.label\n",
        "        decision['code2'] = code2.label\n",
        "        decision['similarity'] = similarity\n",
        "\n",
        "        return decision\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating merge: {str(e)}\")\n",
        "        return {\n",
        "            'code1': code1.label,\n",
        "            'code2': code2.label,\n",
        "            'should_merge': False,\n",
        "            'rationale': 'Error in evaluation'\n",
        "        }\n",
        "\n",
        "def merge_codes(codebook: Dict, code1: str, code2: str,\n",
        "                merged_label: str, merged_definition: str) -> Dict:\n",
        "    \"\"\"Merge two codes into one\"\"\"\n",
        "    if code1 in codebook and code2 in codebook:\n",
        "        # Create merged code\n",
        "        merged_code = codebook[code1]\n",
        "        merged_code.label = sanitize_code_label(merged_label)\n",
        "        merged_code.definition = merged_definition\n",
        "\n",
        "        # Combine examples\n",
        "        merged_code.examples.extend(codebook[code2].examples)\n",
        "        merged_code.frequency += codebook[code2].frequency\n",
        "\n",
        "        # Combine source documents\n",
        "        merged_code.source_documents = list(set(\n",
        "            merged_code.source_documents + codebook[code2].source_documents\n",
        "        ))\n",
        "\n",
        "        # Add merge note\n",
        "        merged_code.notes.append({\n",
        "            'date': datetime.now().isoformat(),\n",
        "            'note': f\"Merged with {code2}\",\n",
        "            'context': 'Conceptual similarity detected'\n",
        "        })\n",
        "\n",
        "        # Remove old codes and add merged\n",
        "        del codebook[code1]\n",
        "        del codebook[code2]\n",
        "        codebook[merged_code.label] = merged_code\n",
        "\n",
        "    return codebook"
      ],
      "metadata": {
        "id": "P6rxPKiKtKxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quality Assessment\n",
        "\n",
        "\n",
        "Validate code completeness and assess conceptual reliability following established methodological guidelines. Generate metrics for definition quality, example sufficiency, and overall codebook integrity."
      ],
      "metadata": {
        "id": "c4Ga7ehetOwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This block validates the quality of extracted codes\n",
        "\n",
        "def validate_codes(codebook: Dict[str, CodeEntry]) -> Dict:\n",
        "    \"\"\"Validate each code meets quality criteria\"\"\"\n",
        "\n",
        "    validation_results = {\n",
        "        'total_codes': len(codebook),\n",
        "        'issues_by_code': {},\n",
        "        'summary': {\n",
        "            'missing_definitions': 0,\n",
        "            'short_definitions': 0,\n",
        "            'missing_inclusion': 0,\n",
        "            'missing_exclusion': 0,\n",
        "            'insufficient_examples': 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if len(codebook) == 0:\n",
        "        validation_results['quality_score'] = 0.0\n",
        "        return validation_results\n",
        "\n",
        "    for label, code in codebook.items():\n",
        "        issues = []\n",
        "\n",
        "        # Check definition\n",
        "        if not code.definition:\n",
        "            issues.append(\"Missing definition\")\n",
        "            validation_results['summary']['missing_definitions'] += 1\n",
        "        elif len(code.definition) < Config.MIN_DEFINITION_LENGTH:\n",
        "            issues.append(f\"Definition too short ({len(code.definition)} chars)\")\n",
        "            validation_results['summary']['short_definitions'] += 1\n",
        "\n",
        "        # Check criteria\n",
        "        if not code.inclusion_criteria:\n",
        "            issues.append(\"Missing inclusion criteria\")\n",
        "            validation_results['summary']['missing_inclusion'] += 1\n",
        "\n",
        "        if not code.exclusion_criteria:\n",
        "            issues.append(\"Missing exclusion criteria\")\n",
        "            validation_results['summary']['missing_exclusion'] += 1\n",
        "\n",
        "        # Check examples\n",
        "        if len(code.examples) < Config.MIN_EXAMPLES_PER_CODE:\n",
        "            issues.append(f\"Insufficient examples ({len(code.examples)})\")\n",
        "            validation_results['summary']['insufficient_examples'] += 1\n",
        "\n",
        "        if issues:\n",
        "            validation_results['issues_by_code'][label] = issues\n",
        "\n",
        "    validation_results['quality_score'] = 1 - (\n",
        "        len(validation_results['issues_by_code']) / len(codebook)\n",
        "    )\n",
        "\n",
        "    return validation_results\n",
        "\n",
        "def assess_conceptual_distinctness(codebook: Dict[str, CodeEntry]) -> Dict:\n",
        "    \"\"\"Assess how conceptually distinct codes are from each other\"\"\"\n",
        "\n",
        "    if len(codebook) < 2:\n",
        "        return {'average_distinctness': 1.0, 'min_distinctness': 1.0, 'overlap_pairs': []}\n",
        "\n",
        "    # Create embeddings for all codes\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    code_labels = list(codebook.keys())\n",
        "\n",
        "    code_representations = []\n",
        "    for label in code_labels:\n",
        "        code = codebook[label]\n",
        "        # Combine all information about the code\n",
        "        representation = f\"{label} {code.definition} \"\n",
        "        representation += \" \".join(code.inclusion_criteria)\n",
        "        representation += \" \".join(code.exclusion_criteria)\n",
        "        code_representations.append(representation)\n",
        "\n",
        "    embeddings = model.encode(code_representations)\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "    # Calculate distinctness metrics\n",
        "    overlap_pairs = []\n",
        "    distinctness_scores = []\n",
        "\n",
        "    for i in range(len(code_labels)):\n",
        "        max_similarity = 0\n",
        "        for j in range(len(code_labels)):\n",
        "            if i != j:\n",
        "                max_similarity = max(max_similarity, similarity_matrix[i][j])\n",
        "                if similarity_matrix[i][j] > 0.7:  # Concerning overlap\n",
        "                    overlap_pairs.append({\n",
        "                        'code1': code_labels[i],\n",
        "                        'code2': code_labels[j],\n",
        "                        'similarity': float(similarity_matrix[i][j])\n",
        "                    })\n",
        "\n",
        "        distinctness_scores.append(1 - max_similarity)\n",
        "\n",
        "    return {\n",
        "        'average_distinctness': float(np.mean(distinctness_scores)),\n",
        "        'min_distinctness': float(np.min(distinctness_scores)),\n",
        "        'overlap_pairs': overlap_pairs[:10]  # Top 10 overlapping pairs\n",
        "    }"
      ],
      "metadata": {
        "id": "TT9KkjoDtQDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version Control\n",
        "\n",
        "Implement semantic versioning and change tracking for the developing codebook. Maintain detailed changelog and create snapshots at key development milestones to ensure reproducibility and documentation.\n"
      ],
      "metadata": {
        "id": "F382wNxltVRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This block implements semantic versioning for the codebook\n",
        "\n",
        "class CodebookVersionControl:\n",
        "    \"\"\"Implement semantic versioning and change tracking\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.current_version = \"1.0.0\"\n",
        "        self.changelog = []\n",
        "        self.version_history = {}\n",
        "\n",
        "    def save_version(self, codebook: Dict[str, CodeEntry], change_description: str):\n",
        "        \"\"\"Save a version with changelog entry\"\"\"\n",
        "\n",
        "        # Create version snapshot\n",
        "        snapshot = {\n",
        "            'version': self.current_version,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'num_codes': len(codebook),\n",
        "            'change_description': change_description,\n",
        "            'codebook_snapshot': self._serialize_codebook(codebook)\n",
        "        }\n",
        "\n",
        "        # Calculate checksum\n",
        "        checksum = hashlib.md5(\n",
        "            json.dumps(snapshot['codebook_snapshot'], sort_keys=True).encode()\n",
        "        ).hexdigest()\n",
        "        snapshot['checksum'] = checksum\n",
        "\n",
        "        # Save to version history\n",
        "        filename = f\"{Config.VERSION_PATH}codebook_v{self.current_version}.json\"\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(snapshot, f, indent=2)\n",
        "\n",
        "        # Update changelog\n",
        "        self.changelog.append({\n",
        "            'version': self.current_version,\n",
        "            'date': datetime.now().isoformat(),\n",
        "            'changes': change_description,\n",
        "            'checksum': checksum\n",
        "        })\n",
        "\n",
        "        # Save changelog\n",
        "        with open(f\"{Config.VERSION_PATH}CHANGELOG.json\", 'w') as f:\n",
        "            json.dump(self.changelog, f, indent=2)\n",
        "\n",
        "        print(f\"Saved version {self.current_version}: {change_description}\")\n",
        "\n",
        "    def increment_version(self, change_type: str = \"patch\"):\n",
        "        \"\"\"Increment version number (major.minor.patch)\"\"\"\n",
        "        major, minor, patch = map(int, self.current_version.split('.'))\n",
        "\n",
        "        if change_type == \"major\":\n",
        "            major += 1\n",
        "            minor = 0\n",
        "            patch = 0\n",
        "        elif change_type == \"minor\":\n",
        "            minor += 1\n",
        "            patch = 0\n",
        "        else:  # patch\n",
        "            patch += 1\n",
        "\n",
        "        self.current_version = f\"{major}.{minor}.{patch}\"\n",
        "        return self.current_version\n",
        "\n",
        "    def _serialize_codebook(self, codebook: Dict[str, CodeEntry]) -> Dict:\n",
        "        \"\"\"Convert CodeEntry objects to JSON-serializable format\"\"\"\n",
        "        serialized = {}\n",
        "        for label, code in codebook.items():\n",
        "            serialized[label] = {\n",
        "                'label': code.label,\n",
        "                'definition': code.definition,\n",
        "                'inclusion_criteria': code.inclusion_criteria,\n",
        "                'exclusion_criteria': code.exclusion_criteria,\n",
        "                'examples': code.examples,\n",
        "                'notes': code.notes,\n",
        "                'source_documents': code.source_documents,\n",
        "                'frequency': code.frequency,\n",
        "                'created_date': code.created_date.isoformat(),\n",
        "                'last_modified': code.last_modified.isoformat(),\n",
        "                'version': code.version\n",
        "            }\n",
        "        return serialized\n",
        "\n",
        "# Initialize version control\n",
        "version_control = CodebookVersionControl()\n",
        "print(\"‚úì Version control initialized\")"
      ],
      "metadata": {
        "id": "EqwDz25RtYm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export and Documentation\n",
        "\n",
        "Generate codebook outputs in multiple formats compatible with qualitative analysis software. Export includes CSV, JSON, Markdown documentation, and formats specifically designed for ATLAS.ti and NVivo integration."
      ],
      "metadata": {
        "id": "i23lydrTtbjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This block handles exporting the codebook in various formats\n",
        "\n",
        "def export_codebook_formats(codebook: Dict[str, CodeEntry],\n",
        "                          assessment_report: Dict,\n",
        "                          purpose_config: Dict):\n",
        "    \"\"\"\n",
        "    Step 6: Format for human and machine readability\n",
        "    Export in multiple formats as recommended\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. CSV format (for quantitative linkage)\n",
        "    export_to_csv(codebook)\n",
        "\n",
        "    # 2. JSON format (for software integration)\n",
        "    export_to_json(codebook, assessment_report)\n",
        "\n",
        "    # 3. Human-readable markdown\n",
        "    export_to_markdown(codebook, assessment_report, purpose_config)\n",
        "\n",
        "    # 4. ATLAS.ti compatible format\n",
        "    export_to_atlas_format(codebook)\n",
        "\n",
        "    # 5. NVivo compatible format\n",
        "    export_to_nvivo_format(codebook)\n",
        "\n",
        "    print(f\"\\nExported codebook in 5 formats to {Config.OUTPUT_PATH}\")\n",
        "\n",
        "def export_to_csv(codebook: Dict[str, CodeEntry]):\n",
        "    \"\"\"Export as CSV with proper formatting\"\"\"\n",
        "\n",
        "    rows = []\n",
        "    for label, code in codebook.items():\n",
        "        row = {\n",
        "            'code_label': label,\n",
        "            'definition': code.definition,\n",
        "            'inclusion_criteria': '; '.join(code.inclusion_criteria),\n",
        "            'exclusion_criteria': '; '.join(code.exclusion_criteria),\n",
        "            'example_1': code.examples[0]['text'] if code.examples else '',\n",
        "            'example_2': code.examples[1]['text'] if len(code.examples) > 1 else '',\n",
        "            'frequency': code.frequency,\n",
        "            'source_documents': '; '.join(code.source_documents),\n",
        "            'created_date': code.created_date.strftime('%Y-%m-%d'),\n",
        "            'version': code.version\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(f\"{Config.OUTPUT_PATH}codebook.csv\", index=False)\n",
        "\n",
        "def export_to_json(codebook: Dict[str, CodeEntry], assessment_report: Dict):\n",
        "    \"\"\"Export as JSON for software integration\"\"\"\n",
        "\n",
        "    json_export = {\n",
        "        'metadata': {\n",
        "            'version': version_control.current_version,\n",
        "            'created': datetime.now().isoformat(),\n",
        "            'total_codes': len(codebook),\n",
        "            'quality_score': assessment_report['validation']['quality_score']\n",
        "        },\n",
        "        'codes': {}\n",
        "    }\n",
        "\n",
        "    for label, code in codebook.items():\n",
        "        json_export['codes'][label] = {\n",
        "            'definition': code.definition,\n",
        "            'inclusion': code.inclusion_criteria,\n",
        "            'exclusion': code.exclusion_criteria,\n",
        "            'examples': [ex['text'] for ex in code.examples],\n",
        "            'frequency': code.frequency,\n",
        "            'sources': code.source_documents\n",
        "        }\n",
        "\n",
        "    with open(f\"{Config.OUTPUT_PATH}codebook.json\", 'w') as f:\n",
        "        json.dump(json_export, f, indent=2)\n",
        "\n",
        "def export_to_markdown(codebook: Dict[str, CodeEntry],\n",
        "                      assessment_report: Dict,\n",
        "                      purpose_config: Dict):\n",
        "    \"\"\"Generate comprehensive markdown documentation\"\"\"\n",
        "\n",
        "    md_content = f\"\"\"# Codebook Documentation\n",
        "\n",
        "**Version**: {version_control.current_version}\n",
        "**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
        "**Purpose**: {purpose_config['purpose']}\n",
        "**Epistemological Stance**: {purpose_config['epistemological_stance']}\n",
        "\n",
        "## Overview\n",
        "\n",
        "This codebook was developed through systematic extraction from {len(set().union(*[code.source_documents for code in codebook.values()]))} source documents using a {Config.CODING_STRATEGY} coding approach.\n",
        "\n",
        "### Statistics\n",
        "- **Total Codes**: {len(codebook)}\n",
        "- **Average Frequency**: {np.mean([code.frequency for code in codebook.values()]):.1f}\n",
        "- **Conceptual Distinctness**: {assessment_report['reliability'].get('average_distinctness', 'N/A')}\n",
        "\n",
        "## Code Definitions\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    # Sort codes alphabetically\n",
        "    for label in sorted(codebook.keys()):\n",
        "        code = codebook[label]\n",
        "\n",
        "        md_content += f\"\"\"### {label}\n",
        "\n",
        "**Definition**: {code.definition}\n",
        "\n",
        "**Frequency**: {code.frequency} occurrences in {len(code.source_documents)} documents\n",
        "\n",
        "**When to use this code**:\n",
        "\"\"\"\n",
        "        for criterion in code.inclusion_criteria:\n",
        "            md_content += f\"- {criterion}\\n\"\n",
        "\n",
        "        md_content += \"\\n**When NOT to use this code**:\\n\"\n",
        "        for criterion in code.exclusion_criteria:\n",
        "            md_content += f\"- {criterion}\\n\"\n",
        "\n",
        "        md_content += \"\\n**Example applications**:\\n\\n\"\n",
        "        for i, example in enumerate(code.examples[:2], 1):\n",
        "            md_content += f'{i}. \"{example[\"text\"][:200]}...\" (*{example[\"source\"]}*)\\n\\n'\n",
        "\n",
        "        if code.notes:\n",
        "            md_content += \"**Notes**:\\n\"\n",
        "            for note in code.notes[-2:]:  # Last 2 notes\n",
        "                md_content += f\"- {note['date'][:10]}: {note['note']}\\n\"\n",
        "\n",
        "        md_content += \"\\n---\\n\\n\"\n",
        "\n",
        "    # Add changelog\n",
        "    md_content += \"## Version History\\n\\n\"\n",
        "    for entry in version_control.changelog[-5:]:  # Last 5 changes\n",
        "        md_content += f\"- **v{entry['version']}** ({entry['date'][:10]}): {entry['changes']}\\n\"\n",
        "\n",
        "    with open(f\"{Config.OUTPUT_PATH}codebook_documentation.md\", 'w') as f:\n",
        "        f.write(md_content)\n",
        "\n",
        "def export_to_atlas_format(codebook: Dict[str, CodeEntry]):\n",
        "    \"\"\"Export in ATLAS.ti compatible format\"\"\"\n",
        "    # Simplified XML format for ATLAS.ti\n",
        "    atlas_export = []\n",
        "    for label, code in codebook.items():\n",
        "        atlas_export.append({\n",
        "            'name': label,\n",
        "            'comment': code.definition,\n",
        "            'examples': [ex['text'] for ex in code.examples]\n",
        "        })\n",
        "\n",
        "    with open(f\"{Config.OUTPUT_PATH}codebook_atlas.json\", 'w') as f:\n",
        "        json.dump(atlas_export, f, indent=2)\n",
        "\n",
        "def export_to_nvivo_format(codebook: Dict[str, CodeEntry]):\n",
        "    \"\"\"Export in NVivo compatible format\"\"\"\n",
        "    # Simplified format for NVivo\n",
        "    nvivo_export = []\n",
        "    for label, code in codebook.items():\n",
        "        nvivo_export.append({\n",
        "            'Name': label,\n",
        "            'Description': code.definition,\n",
        "            'Files': ', '.join(code.source_documents)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(nvivo_export)\n",
        "    df.to_csv(f\"{Config.OUTPUT_PATH}codebook_nvivo.csv\", index=False)"
      ],
      "metadata": {
        "id": "2VhoXGRAtcdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quality Reporting\n",
        "\n",
        "Create detailed quality assurance reports and usage guidelines for the finalized codebook. Generate metrics, recommendations, and documentation to support peer review and research team adoption."
      ],
      "metadata": {
        "id": "I410nD41te96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This block generates comprehensive quality reports\n",
        "\n",
        "def generate_quality_report(codebook: Dict[str, CodeEntry],\n",
        "                          assessment_report: Dict,\n",
        "                          extraction_log: List):\n",
        "    \"\"\"Generate comprehensive quality assurance report\"\"\"\n",
        "\n",
        "    report = {\n",
        "        'metadata': {\n",
        "            'version': version_control.current_version,\n",
        "            'generated': datetime.now().isoformat(),\n",
        "            'total_codes': len(codebook)\n",
        "        },\n",
        "        'quality_metrics': {\n",
        "            'overall_quality': assessment_report['validation']['quality_score'],\n",
        "            'conceptual_distinctness': assessment_report['reliability']['average_distinctness'],\n",
        "            'validation_issues': assessment_report['validation']['summary']\n",
        "        },\n",
        "        'extraction_summary': {\n",
        "            'total_extractions': len(extraction_log),\n",
        "            'documents_processed': len(set(e['document'] for e in extraction_log))\n",
        "        },\n",
        "        'recommendations': []\n",
        "    }\n",
        "\n",
        "    # Generate recommendations\n",
        "    if assessment_report['validation']['summary']['missing_definitions'] > 0:\n",
        "        report['recommendations'].append(\n",
        "            \"Add definitions to all codes before finalizing\"\n",
        "        )\n",
        "\n",
        "    if assessment_report['reliability']['average_distinctness'] < 0.8:\n",
        "        report['recommendations'].append(\n",
        "            \"Review overlapping codes for potential mergers\"\n",
        "        )\n",
        "\n",
        "    with open(f\"{Config.OUTPUT_PATH}quality_report.json\", 'w') as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "def create_usage_guidelines(codebook: Dict[str, CodeEntry], purpose_config: Dict):\n",
        "    \"\"\"Create guidelines for using the codebook\"\"\"\n",
        "\n",
        "    guidelines = f\"\"\"# Codebook Usage Guidelines\n",
        "\n",
        "## Purpose\n",
        "{purpose_config['purpose']}\n",
        "\n",
        "## How to Apply These Codes\n",
        "\n",
        "1. **Read the full definition** before applying any code\n",
        "2. **Check inclusion criteria** - the text must meet these conditions\n",
        "3. **Check exclusion criteria** - if any apply, do not use the code\n",
        "4. **Reference the examples** when uncertain\n",
        "5. **Document edge cases** in your coding notes\n",
        "\n",
        "## Code Application Rules\n",
        "\n",
        "- Codes are conceptually mutually exclusive\n",
        "- Multiple codes may be applied to the same text segment if justified\n",
        "- When in doubt, refer to the examples provided\n",
        "- Record rationale for difficult coding decisions\n",
        "\n",
        "## Quality Checks\n",
        "\n",
        "- Aim for consistency across coders\n",
        "- Regular team meetings to discuss edge cases\n",
        "- Update coding notes for future reference\n",
        "- Version control any modifications\n",
        "\n",
        "## Contact\n",
        "For questions about code definitions or applications, consult the codebook documentation.\n",
        "\"\"\"\n",
        "\n",
        "    with open(f\"{Config.OUTPUT_PATH}usage_guidelines.md\", 'w') as f:\n",
        "        f.write(guidelines)"
      ],
      "metadata": {
        "id": "VnKQ5i_MtiUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Pipeline\n",
        "\n",
        "Orchestrate the complete codebook development workflow from document analysis through final export. This main execution function coordinates all previous components following methodological best practices."
      ],
      "metadata": {
        "id": "SRybfXmJ_7cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the main execution pipeline that orchestrates the entire codebook development process\n",
        "\n",
        "def develop_codebook_pipeline():\n",
        "    \"\"\"\n",
        "    Main pipeline following the 8-step best practice guide\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== Codebook Development System ===\")\n",
        "    print(\"Following methodological best practices\\n\")\n",
        "\n",
        "    # Check if documents are uploaded\n",
        "    if 'uploaded_documents' not in globals() or not uploaded_documents:\n",
        "        print(\"\\nNo documents found. Please run the file upload cell above first!\")\n",
        "        return\n",
        "\n",
        "    # Step 1: Define purpose and scope\n",
        "    print(\"Step 1: Defining codebook purpose and scope...\")\n",
        "    purpose_config = define_codebook_purpose()\n",
        "    print(f\"Purpose: {purpose_config['purpose']}\")\n",
        "    print(f\"Epistemological stance: {purpose_config['epistemological_stance']}\")\n",
        "\n",
        "    # Step 2: Analyze document types\n",
        "    print(\"\\nStep 2: Analyzing uploaded documents...\")\n",
        "    document_analysis = analyze_document_types(uploaded_documents)\n",
        "    print(f\"Analyzed {len(document_analysis)} documents\")\n",
        "\n",
        "    # Save initial version\n",
        "    version_control.save_version({}, \"Initial setup - document analysis complete\")\n",
        "\n",
        "    # Step 3: Extract initial codes\n",
        "    print(\"\\nStep 3: Building initial code set...\")\n",
        "    initial_codebook, extraction_log = extract_initial_codes(\n",
        "        uploaded_documents,\n",
        "        Config.CODING_STRATEGY\n",
        "    )\n",
        "    print(f\"Extracted {len(initial_codebook)} initial codes\")\n",
        "\n",
        "    # Save version after initial extraction\n",
        "    version_control.increment_version(\"minor\")\n",
        "    version_control.save_version(initial_codebook, \"Initial code extraction complete\")\n",
        "\n",
        "    # Step 4: Refine and assess reliability\n",
        "    print(\"\\nStep 4: Training codes and assessing reliability...\")\n",
        "    refined_codebook, assessment_report = refine_and_assess_reliability(initial_codebook)\n",
        "    print(f\"Refined to {len(refined_codebook)} codes\")\n",
        "    print(f\"Quality score: {assessment_report['validation']['quality_score']:.2f}\")\n",
        "\n",
        "    # Step 5: Finalize and freeze\n",
        "    if assessment_report['validation']['quality_score'] >= 0.7:\n",
        "        print(\"\\nStep 5: Freezing codebook (quality threshold met)\")\n",
        "        version_control.increment_version(\"major\")\n",
        "        version_control.save_version(refined_codebook, \"Codebook frozen - ready for use\")\n",
        "    else:\n",
        "        print(\"\\nStep 5: Codebook needs improvement before freezing\")\n",
        "        print(\"Issues to address:\")\n",
        "        for issue_type, count in assessment_report['validation']['summary'].items():\n",
        "            if count > 0:\n",
        "                print(f\"  - {issue_type}: {count} codes\")\n",
        "\n",
        "    # Step 6: Export in multiple formats\n",
        "    print(\"\\nStep 6: Formatting for human and machine readability...\")\n",
        "    export_codebook_formats(refined_codebook, assessment_report, purpose_config)\n",
        "\n",
        "    # Step 7: Generate quality report\n",
        "    print(\"\\nStep 7: Generating quality assurance report...\")\n",
        "    generate_quality_report(refined_codebook, assessment_report, extraction_log)\n",
        "\n",
        "    # Step 8: Create usage guidelines\n",
        "    print(\"\\nStep 8: Creating usage guidelines...\")\n",
        "    create_usage_guidelines(refined_codebook, purpose_config)\n",
        "\n",
        "    print(\"\\n=== Codebook Development Complete ===\")\n",
        "    print(f\"Version: {version_control.current_version}\")\n",
        "    print(f\"Total codes: {len(refined_codebook)}\")\n",
        "    print(f\"Quality score: {assessment_report['validation']['quality_score']:.2f}\")\n",
        "    print(f\"\\nOutputs saved to: {Config.OUTPUT_PATH}\")\n",
        "\n",
        "    return refined_codebook, assessment_report\n",
        "\n",
        "# Run the pipeline\n",
        "print(\"Ready to run codebook development!\")\n",
        "print(\"To start, execute: develop_codebook_pipeline()\")"
      ],
      "metadata": {
        "id": "0Ambj8db_9P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Codebook Development\n",
        "\n",
        "Run the complete codebook development process with your uploaded documents and configured parameters. Monitor progress through each development stage and generate final outputs ready for research application.\n"
      ],
      "metadata": {
        "id": "hLFISr5IAAGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute this cell to run the complete codebook development process\n",
        "print(\"üöÄ Starting Codebook Development Pipeline...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    result = develop_codebook_pipeline()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üéâ CODEBOOK DEVELOPMENT COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"‚úÖ Pipeline executed successfully\")\n",
        "    print(\"üìÅ Check the output directory for your generated files:\")\n",
        "    print(f\"   {Config.OUTPUT_PATH}\")\n",
        "    print(\"\\nüìã Generated files include:\")\n",
        "    print(\"   ‚Ä¢ codebook.csv - Main codebook data\")\n",
        "    print(\"   ‚Ä¢ codebook.json - JSON format for software integration\")\n",
        "    print(\"   ‚Ä¢ codebook_documentation.md - Human-readable documentation\")\n",
        "    print(\"   ‚Ä¢ codebook_atlas.json - ATLAS.ti compatible format\")\n",
        "    print(\"   ‚Ä¢ codebook_nvivo.csv - NVivo compatible format\")\n",
        "    print(\"   ‚Ä¢ quality_report.json - Quality assessment metrics\")\n",
        "    print(\"   ‚Ä¢ usage_guidelines.md - Guidelines for using the codebook\")\n",
        "    print(\"\\nüèÅ You can now use your codebook for qualitative analysis!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚ùå PIPELINE EXECUTION FAILED!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    print(\"\\nüîß Troubleshooting steps:\")\n",
        "    print(\"   1. Check that all configuration parameters are set\")\n",
        "    print(\"   2. Verify that documents are uploaded\")\n",
        "    print(\"   3. Ensure API key is valid and working\")\n",
        "    print(\"   4. Review any error messages above\")\n",
        "    print(\"\\nüí° Try running individual pipeline steps to isolate the issue.\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "QDn-Kog_AByN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}